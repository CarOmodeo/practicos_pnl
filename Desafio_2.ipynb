{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOnRS/4BfF9qrAkzElX+TD3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarOmodeo/practicos_pnl/blob/main/Desafio_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Desafio 2 - Vectores con Gensim"
      ],
      "metadata": {
        "id": "6cY0L_-tnjA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alumno: Carolina Pérez Omodeo\n"
      ],
      "metadata": {
        "id": "bW3RHPyinqRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Crear sus propios vectores con Gensim basado en lo visto en clase con otro dataset.\n",
        "- Probar términos de interés y explicar similitudes en el espacio de embeddings (sacar conclusiones entre palabras similitudes y diferencias).\n",
        "- Graficarlos.\n",
        "- Obtener conclusiones."
      ],
      "metadata": {
        "id": "dhip0hDln2z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "Yi2Xh9XQnuty"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datos\n",
        "\n",
        "Se utilizaran los libros de \"Orgullo y Prejuicio\" de Jane Austen para desarrollar el desafio."
      ],
      "metadata": {
        "id": "p-yB9cQaCoBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "rDcz8uizDUaT",
        "outputId": "863cef99-0a14-4e69-d60f-f90d5dc074aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e9011f3a-08b7-49c6-95e0-2904eee28691\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e9011f3a-08b7-49c6-95e0-2904eee28691\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving orgullo_y_prejuicio.txt to orgullo_y_prejuicio.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('orgullo_y_prejuicio.txt', sep='/n', header=None, engine = 'python')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6Up6h7wBD96e",
        "outputId": "661c79eb-15fa-4fe2-eba8-97571390e3c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   0\n",
              "0                                Orgullo y Prejuicio\n",
              "1                                        Jane Austen\n",
              "2                                         Capítulo I\n",
              "3  Es una verdad mundialmente reconocida que un h...\n",
              "4  Sin embargo, poco se sabe de los sentimientos ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1832b3e3-2763-4a08-8db0-93968c3645ac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Orgullo y Prejuicio</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jane Austen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Capítulo I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Es una verdad mundialmente reconocida que un h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sin embargo, poco se sabe de los sentimientos ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1832b3e3-2763-4a08-8db0-93968c3645ac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1832b3e3-2763-4a08-8db0-93968c3645ac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1832b3e3-2763-4a08-8db0-93968c3645ac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ef7af1b9-f3ad-472b-95eb-020350bf61cf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ef7af1b9-f3ad-472b-95eb-020350bf61cf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ef7af1b9-f3ad-472b-95eb-020350bf61cf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2196,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2190,\n        \"samples\": [\n          \"\\u2014Debe poseer todo esto \\u2014agreg\\u00f3 Darcy\\u2014, y a ello hay que a\\u00f1adir algo m\\u00e1s sustancial en el desarrollo de su inteligencia por medio de abundantes lecturas.\",\n          \"Cap\\u00edtulo LI\",\n          \"La se\\u00f1ora Gardiner se fue perpleja a\\u00fan al pensar en el encuentro casual de Elizabeth y su amigo de Derbyshire en dicho lugar. Elizabeth se hab\\u00eda abstenido de pronunciar su nombre, y aquella especie de semiesperanza que la t\\u00eda hab\\u00eda alimentado de que recibir\\u00edan una carta de \\u00e9l al llegar a Longbourn, se hab\\u00eda quedado en nada. Desde su llegada, Elizabeth no hab\\u00eda tenido ninguna carta de Pemberley.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total de documentos: \", df.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS0ZhW7HMpjb",
        "outputId": "cb0d54aa-9589-4cf2-96a9-a0863a64dcaa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de documentos:  2196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa de Procesamiento"
      ],
      "metadata": {
        "id": "DxFUvXYXM7us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# Paso del texto a listas\n",
        "\n",
        "sentence_tokens = []\n",
        "\n",
        "for _, row in df[:None].iterrows():\n",
        "    sentence_tokens.append(text_to_word_sequence(row[0]))"
      ],
      "metadata": {
        "id": "E_aU-8PGMzNr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listas formadas\n",
        "sentence_tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4TnQ-kjN4ez",
        "outputId": "4cb96641-00e9-410f-86ca-3752002d2e35"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['orgullo', 'y', 'prejuicio'],\n",
              " ['jane', 'austen'],\n",
              " ['capítulo', 'i'],\n",
              " ['es',\n",
              "  'una',\n",
              "  'verdad',\n",
              "  'mundialmente',\n",
              "  'reconocida',\n",
              "  'que',\n",
              "  'un',\n",
              "  'hombre',\n",
              "  'soltero',\n",
              "  'poseedor',\n",
              "  'de',\n",
              "  'una',\n",
              "  'gran',\n",
              "  'fortuna',\n",
              "  'necesita',\n",
              "  'una',\n",
              "  'esposa'],\n",
              " ['sin',\n",
              "  'embargo',\n",
              "  'poco',\n",
              "  'se',\n",
              "  'sabe',\n",
              "  'de',\n",
              "  'los',\n",
              "  'sentimientos',\n",
              "  'u',\n",
              "  'opiniones',\n",
              "  'de',\n",
              "  'un',\n",
              "  'hombre',\n",
              "  'de',\n",
              "  'tales',\n",
              "  'condiciones',\n",
              "  'cuando',\n",
              "  'entra',\n",
              "  'a',\n",
              "  'formar',\n",
              "  'parte',\n",
              "  'de',\n",
              "  'un',\n",
              "  'vecindario',\n",
              "  'esta',\n",
              "  'verdad',\n",
              "  'está',\n",
              "  'tan',\n",
              "  'arraigada',\n",
              "  'en',\n",
              "  'las',\n",
              "  'mentes',\n",
              "  'de',\n",
              "  'algunas',\n",
              "  'de',\n",
              "  'las',\n",
              "  'familias',\n",
              "  'que',\n",
              "  'lo',\n",
              "  'rodean',\n",
              "  'que',\n",
              "  'algunas',\n",
              "  'le',\n",
              "  'consideran',\n",
              "  'de',\n",
              "  'su',\n",
              "  'legítima',\n",
              "  'propiedad',\n",
              "  'y',\n",
              "  'otras',\n",
              "  'de',\n",
              "  'la',\n",
              "  'de',\n",
              "  'sus',\n",
              "  'hijas']]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa de creación de vectores (word2vec)"
      ],
      "metadata": {
        "id": "Tc0FRpLXOTru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
        "# Sobrecargamos el callback para poder tener esta información\n",
        "class callback(CallbackAny2Vec):\n",
        "    \"\"\"\n",
        "    Callback to print loss after each epoch\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        loss = model.get_latest_training_loss()\n",
        "        if self.epoch == 0:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
        "        else:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
        "        self.epoch += 1\n",
        "        self.loss_previous_step = loss"
      ],
      "metadata": {
        "id": "mnLGroI5OGtQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea el modelo generador de vectores, con skipgram\n",
        "\n",
        "w2v_model = Word2Vec(min_count=10,      # frecuencia mínima de palabra para incluirla en el vocabulario\n",
        "                     window=10,          # cant de palabras antes y desp de la predicha\n",
        "                     vector_size=200,   # dimensionalidad de los vectores\n",
        "                     negative=3,        # cantidad de negative samples... 0 es no se usa\n",
        "                     workers=2,        # si tienen más cores pueden cambiar este valor\n",
        "                     sg=1)              # modelo 0:CBOW  1:skipgram"
      ],
      "metadata": {
        "id": "3I6BDjQ6OfXv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulario con los tokens\n",
        "w2v_model.build_vocab(sentence_tokens)"
      ],
      "metadata": {
        "id": "GyEXIA80PUS_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cantidad de filas/docs encontradas en el corpus\n",
        "print(\"Cantidad de docs en el corpus:\", w2v_model.corpus_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48hPS3ibP-lJ",
        "outputId": "d8e0641d-dec5-49c8-9fa4-015898f8cf9f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de docs en el corpus: 2196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cantidad de words encontradas en el corpus\n",
        "print(\"Cantidad de words distintas en el corpus:\", len(w2v_model.wv.index_to_key))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekllwCOFQEz-",
        "outputId": "9ca83da9-9afd-4230-fe3f-a60d96108121"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de words distintas en el corpus: 1186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa de entrenamiento de embeddings"
      ],
      "metadata": {
        "id": "DuE5dnt3QMni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el modelo generador de vectores\n",
        "w2v_model.train(sentence_tokens,\n",
        "                 total_examples=w2v_model.corpus_count,\n",
        "                 epochs=1001,\n",
        "                 compute_loss = True,\n",
        "                 callbacks=[callback()]\n",
        "                 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHxASUeZQHn_",
        "outputId": "87bbe472-a079-42d2-eace-eeee0c5f80ed"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0: 675244.4375\n",
            "Loss after epoch 1: 634549.9375\n",
            "Loss after epoch 2: 635147.75\n",
            "Loss after epoch 3: 633993.625\n",
            "Loss after epoch 4: 634684.75\n",
            "Loss after epoch 5: 632958.75\n",
            "Loss after epoch 6: 614878.75\n",
            "Loss after epoch 7: 584730.5\n",
            "Loss after epoch 8: 593767.5\n",
            "Loss after epoch 9: 591055.0\n",
            "Loss after epoch 10: 590627.5\n",
            "Loss after epoch 11: 574995.0\n",
            "Loss after epoch 12: 572076.0\n",
            "Loss after epoch 13: 518686.5\n",
            "Loss after epoch 14: 467907.0\n",
            "Loss after epoch 15: 472050.0\n",
            "Loss after epoch 16: 469433.0\n",
            "Loss after epoch 17: 437469.0\n",
            "Loss after epoch 18: 473432.0\n",
            "Loss after epoch 19: 475287.0\n",
            "Loss after epoch 20: 475521.0\n",
            "Loss after epoch 21: 470192.0\n",
            "Loss after epoch 22: 474858.0\n",
            "Loss after epoch 23: 440421.0\n",
            "Loss after epoch 24: 464873.0\n",
            "Loss after epoch 25: 435424.0\n",
            "Loss after epoch 26: 469449.0\n",
            "Loss after epoch 27: 467680.0\n",
            "Loss after epoch 28: 476523.0\n",
            "Loss after epoch 29: 476554.0\n",
            "Loss after epoch 30: 476778.0\n",
            "Loss after epoch 31: 454961.0\n",
            "Loss after epoch 32: 353156.0\n",
            "Loss after epoch 33: 382830.0\n",
            "Loss after epoch 34: 382050.0\n",
            "Loss after epoch 35: 383942.0\n",
            "Loss after epoch 36: 382500.0\n",
            "Loss after epoch 37: 378256.0\n",
            "Loss after epoch 38: 382882.0\n",
            "Loss after epoch 39: 350208.0\n",
            "Loss after epoch 40: 377302.0\n",
            "Loss after epoch 41: 373958.0\n",
            "Loss after epoch 42: 377396.0\n",
            "Loss after epoch 43: 378848.0\n",
            "Loss after epoch 44: 352954.0\n",
            "Loss after epoch 45: 314558.0\n",
            "Loss after epoch 46: 414006.0\n",
            "Loss after epoch 47: 355322.0\n",
            "Loss after epoch 48: 413966.0\n",
            "Loss after epoch 49: 347682.0\n",
            "Loss after epoch 50: 356838.0\n",
            "Loss after epoch 51: 382980.0\n",
            "Loss after epoch 52: 383468.0\n",
            "Loss after epoch 53: 376120.0\n",
            "Loss after epoch 54: 385178.0\n",
            "Loss after epoch 55: 374598.0\n",
            "Loss after epoch 56: 381412.0\n",
            "Loss after epoch 57: 376066.0\n",
            "Loss after epoch 58: 384388.0\n",
            "Loss after epoch 59: 382924.0\n",
            "Loss after epoch 60: 381658.0\n",
            "Loss after epoch 61: 381732.0\n",
            "Loss after epoch 62: 387138.0\n",
            "Loss after epoch 63: 382652.0\n",
            "Loss after epoch 64: 381190.0\n",
            "Loss after epoch 65: 387080.0\n",
            "Loss after epoch 66: 383836.0\n",
            "Loss after epoch 67: 379278.0\n",
            "Loss after epoch 68: 384970.0\n",
            "Loss after epoch 69: 382116.0\n",
            "Loss after epoch 70: 383984.0\n",
            "Loss after epoch 71: 384930.0\n",
            "Loss after epoch 72: 384022.0\n",
            "Loss after epoch 73: 375580.0\n",
            "Loss after epoch 74: 382216.0\n",
            "Loss after epoch 75: 378448.0\n",
            "Loss after epoch 76: 181446.0\n",
            "Loss after epoch 77: 128508.0\n",
            "Loss after epoch 78: 138744.0\n",
            "Loss after epoch 79: 138692.0\n",
            "Loss after epoch 80: 120480.0\n",
            "Loss after epoch 81: 115584.0\n",
            "Loss after epoch 82: 127728.0\n",
            "Loss after epoch 83: 126164.0\n",
            "Loss after epoch 84: 127276.0\n",
            "Loss after epoch 85: 128412.0\n",
            "Loss after epoch 86: 125052.0\n",
            "Loss after epoch 87: 124580.0\n",
            "Loss after epoch 88: 128012.0\n",
            "Loss after epoch 89: 125896.0\n",
            "Loss after epoch 90: 127060.0\n",
            "Loss after epoch 91: 127600.0\n",
            "Loss after epoch 92: 125736.0\n",
            "Loss after epoch 93: 125476.0\n",
            "Loss after epoch 94: 124200.0\n",
            "Loss after epoch 95: 127300.0\n",
            "Loss after epoch 96: 129116.0\n",
            "Loss after epoch 97: 125100.0\n",
            "Loss after epoch 98: 124424.0\n",
            "Loss after epoch 99: 126640.0\n",
            "Loss after epoch 100: 125404.0\n",
            "Loss after epoch 101: 125180.0\n",
            "Loss after epoch 102: 127528.0\n",
            "Loss after epoch 103: 128596.0\n",
            "Loss after epoch 104: 125504.0\n",
            "Loss after epoch 105: 125800.0\n",
            "Loss after epoch 106: 124464.0\n",
            "Loss after epoch 107: 128808.0\n",
            "Loss after epoch 108: 127224.0\n",
            "Loss after epoch 109: 126856.0\n",
            "Loss after epoch 110: 124788.0\n",
            "Loss after epoch 111: 116948.0\n",
            "Loss after epoch 112: 128684.0\n",
            "Loss after epoch 113: 113080.0\n",
            "Loss after epoch 114: 127692.0\n",
            "Loss after epoch 115: 115644.0\n",
            "Loss after epoch 116: 124864.0\n",
            "Loss after epoch 117: 127880.0\n",
            "Loss after epoch 118: 125192.0\n",
            "Loss after epoch 119: 115128.0\n",
            "Loss after epoch 120: 125872.0\n",
            "Loss after epoch 121: 117428.0\n",
            "Loss after epoch 122: 123976.0\n",
            "Loss after epoch 123: 125816.0\n",
            "Loss after epoch 124: 125868.0\n",
            "Loss after epoch 125: 125788.0\n",
            "Loss after epoch 126: 127860.0\n",
            "Loss after epoch 127: 124312.0\n",
            "Loss after epoch 128: 123384.0\n",
            "Loss after epoch 129: 124148.0\n",
            "Loss after epoch 130: 126228.0\n",
            "Loss after epoch 131: 124720.0\n",
            "Loss after epoch 132: 125604.0\n",
            "Loss after epoch 133: 125724.0\n",
            "Loss after epoch 134: 124800.0\n",
            "Loss after epoch 135: 123348.0\n",
            "Loss after epoch 136: 127312.0\n",
            "Loss after epoch 137: 124380.0\n",
            "Loss after epoch 138: 123560.0\n",
            "Loss after epoch 139: 124656.0\n",
            "Loss after epoch 140: 125488.0\n",
            "Loss after epoch 141: 122692.0\n",
            "Loss after epoch 142: 123684.0\n",
            "Loss after epoch 143: 122976.0\n",
            "Loss after epoch 144: 115964.0\n",
            "Loss after epoch 145: 124864.0\n",
            "Loss after epoch 146: 135392.0\n",
            "Loss after epoch 147: 117912.0\n",
            "Loss after epoch 148: 123340.0\n",
            "Loss after epoch 149: 122360.0\n",
            "Loss after epoch 150: 121452.0\n",
            "Loss after epoch 151: 114152.0\n",
            "Loss after epoch 152: 122284.0\n",
            "Loss after epoch 153: 125848.0\n",
            "Loss after epoch 154: 124460.0\n",
            "Loss after epoch 155: 124260.0\n",
            "Loss after epoch 156: 123952.0\n",
            "Loss after epoch 157: 125508.0\n",
            "Loss after epoch 158: 125124.0\n",
            "Loss after epoch 159: 124076.0\n",
            "Loss after epoch 160: 124948.0\n",
            "Loss after epoch 161: 121180.0\n",
            "Loss after epoch 162: 122984.0\n",
            "Loss after epoch 163: 124016.0\n",
            "Loss after epoch 164: 125172.0\n",
            "Loss after epoch 165: 121172.0\n",
            "Loss after epoch 166: 124428.0\n",
            "Loss after epoch 167: 123620.0\n",
            "Loss after epoch 168: 123512.0\n",
            "Loss after epoch 169: 122516.0\n",
            "Loss after epoch 170: 122832.0\n",
            "Loss after epoch 171: 123000.0\n",
            "Loss after epoch 172: 122212.0\n",
            "Loss after epoch 173: 120272.0\n",
            "Loss after epoch 174: 123572.0\n",
            "Loss after epoch 175: 125032.0\n",
            "Loss after epoch 176: 122752.0\n",
            "Loss after epoch 177: 120580.0\n",
            "Loss after epoch 178: 123516.0\n",
            "Loss after epoch 179: 121824.0\n",
            "Loss after epoch 180: 133384.0\n",
            "Loss after epoch 181: 135716.0\n",
            "Loss after epoch 182: 120940.0\n",
            "Loss after epoch 183: 120180.0\n",
            "Loss after epoch 184: 123088.0\n",
            "Loss after epoch 185: 120628.0\n",
            "Loss after epoch 186: 123068.0\n",
            "Loss after epoch 187: 124960.0\n",
            "Loss after epoch 188: 121208.0\n",
            "Loss after epoch 189: 120776.0\n",
            "Loss after epoch 190: 123496.0\n",
            "Loss after epoch 191: 118432.0\n",
            "Loss after epoch 192: 122260.0\n",
            "Loss after epoch 193: 125492.0\n",
            "Loss after epoch 194: 121968.0\n",
            "Loss after epoch 195: 122400.0\n",
            "Loss after epoch 196: 122340.0\n",
            "Loss after epoch 197: 121588.0\n",
            "Loss after epoch 198: 110912.0\n",
            "Loss after epoch 199: 120492.0\n",
            "Loss after epoch 200: 121068.0\n",
            "Loss after epoch 201: 119856.0\n",
            "Loss after epoch 202: 120076.0\n",
            "Loss after epoch 203: 122372.0\n",
            "Loss after epoch 204: 120568.0\n",
            "Loss after epoch 205: 123276.0\n",
            "Loss after epoch 206: 118220.0\n",
            "Loss after epoch 207: 121716.0\n",
            "Loss after epoch 208: 119960.0\n",
            "Loss after epoch 209: 118724.0\n",
            "Loss after epoch 210: 120600.0\n",
            "Loss after epoch 211: 119480.0\n",
            "Loss after epoch 212: 113856.0\n",
            "Loss after epoch 213: 122328.0\n",
            "Loss after epoch 214: 122924.0\n",
            "Loss after epoch 215: 119732.0\n",
            "Loss after epoch 216: 134184.0\n",
            "Loss after epoch 217: 120276.0\n",
            "Loss after epoch 218: 121488.0\n",
            "Loss after epoch 219: 120692.0\n",
            "Loss after epoch 220: 118588.0\n",
            "Loss after epoch 221: 120792.0\n",
            "Loss after epoch 222: 118956.0\n",
            "Loss after epoch 223: 120272.0\n",
            "Loss after epoch 224: 122060.0\n",
            "Loss after epoch 225: 118492.0\n",
            "Loss after epoch 226: 121196.0\n",
            "Loss after epoch 227: 120704.0\n",
            "Loss after epoch 228: 120336.0\n",
            "Loss after epoch 229: 119252.0\n",
            "Loss after epoch 230: 117864.0\n",
            "Loss after epoch 231: 121324.0\n",
            "Loss after epoch 232: 118080.0\n",
            "Loss after epoch 233: 121740.0\n",
            "Loss after epoch 234: 118280.0\n",
            "Loss after epoch 235: 120268.0\n",
            "Loss after epoch 236: 120464.0\n",
            "Loss after epoch 237: 118768.0\n",
            "Loss after epoch 238: 119740.0\n",
            "Loss after epoch 239: 117464.0\n",
            "Loss after epoch 240: 119648.0\n",
            "Loss after epoch 241: 118352.0\n",
            "Loss after epoch 242: 119932.0\n",
            "Loss after epoch 243: 118448.0\n",
            "Loss after epoch 244: 115764.0\n",
            "Loss after epoch 245: 119712.0\n",
            "Loss after epoch 246: 119972.0\n",
            "Loss after epoch 247: 130756.0\n",
            "Loss after epoch 248: 110188.0\n",
            "Loss after epoch 249: 127504.0\n",
            "Loss after epoch 250: 120804.0\n",
            "Loss after epoch 251: 129964.0\n",
            "Loss after epoch 252: 118344.0\n",
            "Loss after epoch 253: 108872.0\n",
            "Loss after epoch 254: 118448.0\n",
            "Loss after epoch 255: 115640.0\n",
            "Loss after epoch 256: 118128.0\n",
            "Loss after epoch 257: 117992.0\n",
            "Loss after epoch 258: 118148.0\n",
            "Loss after epoch 259: 117464.0\n",
            "Loss after epoch 260: 118772.0\n",
            "Loss after epoch 261: 108036.0\n",
            "Loss after epoch 262: 116668.0\n",
            "Loss after epoch 263: 116252.0\n",
            "Loss after epoch 264: 116488.0\n",
            "Loss after epoch 265: 116496.0\n",
            "Loss after epoch 266: 116904.0\n",
            "Loss after epoch 267: 119120.0\n",
            "Loss after epoch 268: 117888.0\n",
            "Loss after epoch 269: 116760.0\n",
            "Loss after epoch 270: 118256.0\n",
            "Loss after epoch 271: 117644.0\n",
            "Loss after epoch 272: 116496.0\n",
            "Loss after epoch 273: 116544.0\n",
            "Loss after epoch 274: 117236.0\n",
            "Loss after epoch 275: 117968.0\n",
            "Loss after epoch 276: 115080.0\n",
            "Loss after epoch 277: 115532.0\n",
            "Loss after epoch 278: 115256.0\n",
            "Loss after epoch 279: 114840.0\n",
            "Loss after epoch 280: 117904.0\n",
            "Loss after epoch 281: 106832.0\n",
            "Loss after epoch 282: 116652.0\n",
            "Loss after epoch 283: 105564.0\n",
            "Loss after epoch 284: 106632.0\n",
            "Loss after epoch 285: 119076.0\n",
            "Loss after epoch 286: 104676.0\n",
            "Loss after epoch 287: 114796.0\n",
            "Loss after epoch 288: 117376.0\n",
            "Loss after epoch 289: 116084.0\n",
            "Loss after epoch 290: 112328.0\n",
            "Loss after epoch 291: 116176.0\n",
            "Loss after epoch 292: 116884.0\n",
            "Loss after epoch 293: 114864.0\n",
            "Loss after epoch 294: 114352.0\n",
            "Loss after epoch 295: 114800.0\n",
            "Loss after epoch 296: 115220.0\n",
            "Loss after epoch 297: 115824.0\n",
            "Loss after epoch 298: 114260.0\n",
            "Loss after epoch 299: 116528.0\n",
            "Loss after epoch 300: 113760.0\n",
            "Loss after epoch 301: 113456.0\n",
            "Loss after epoch 302: 115452.0\n",
            "Loss after epoch 303: 114368.0\n",
            "Loss after epoch 304: 113036.0\n",
            "Loss after epoch 305: 115308.0\n",
            "Loss after epoch 306: 113020.0\n",
            "Loss after epoch 307: 114516.0\n",
            "Loss after epoch 308: 114304.0\n",
            "Loss after epoch 309: 113860.0\n",
            "Loss after epoch 310: 107396.0\n",
            "Loss after epoch 311: 115452.0\n",
            "Loss after epoch 312: 114968.0\n",
            "Loss after epoch 313: 112736.0\n",
            "Loss after epoch 314: 113848.0\n",
            "Loss after epoch 315: 93680.0\n",
            "Loss after epoch 316: 125268.0\n",
            "Loss after epoch 317: 103408.0\n",
            "Loss after epoch 318: 104848.0\n",
            "Loss after epoch 319: 116096.0\n",
            "Loss after epoch 320: 122836.0\n",
            "Loss after epoch 321: 112712.0\n",
            "Loss after epoch 322: 114460.0\n",
            "Loss after epoch 323: 111124.0\n",
            "Loss after epoch 324: 115696.0\n",
            "Loss after epoch 325: 114112.0\n",
            "Loss after epoch 326: 111032.0\n",
            "Loss after epoch 327: 113048.0\n",
            "Loss after epoch 328: 113632.0\n",
            "Loss after epoch 329: 113192.0\n",
            "Loss after epoch 330: 114244.0\n",
            "Loss after epoch 331: 114936.0\n",
            "Loss after epoch 332: 113192.0\n",
            "Loss after epoch 333: 110840.0\n",
            "Loss after epoch 334: 111916.0\n",
            "Loss after epoch 335: 105792.0\n",
            "Loss after epoch 336: 115168.0\n",
            "Loss after epoch 337: 113396.0\n",
            "Loss after epoch 338: 111740.0\n",
            "Loss after epoch 339: 110784.0\n",
            "Loss after epoch 340: 113076.0\n",
            "Loss after epoch 341: 112372.0\n",
            "Loss after epoch 342: 111004.0\n",
            "Loss after epoch 343: 113628.0\n",
            "Loss after epoch 344: 103536.0\n",
            "Loss after epoch 345: 110128.0\n",
            "Loss after epoch 346: 111760.0\n",
            "Loss after epoch 347: 112708.0\n",
            "Loss after epoch 348: 111916.0\n",
            "Loss after epoch 349: 103896.0\n",
            "Loss after epoch 350: 123324.0\n",
            "Loss after epoch 351: 110896.0\n",
            "Loss after epoch 352: 94500.0\n",
            "Loss after epoch 353: 105460.0\n",
            "Loss after epoch 354: 110832.0\n",
            "Loss after epoch 355: 114072.0\n",
            "Loss after epoch 356: 97796.0\n",
            "Loss after epoch 357: 2040.0\n",
            "Loss after epoch 358: 1880.0\n",
            "Loss after epoch 359: 1872.0\n",
            "Loss after epoch 360: 1928.0\n",
            "Loss after epoch 361: 2216.0\n",
            "Loss after epoch 362: 1976.0\n",
            "Loss after epoch 363: 1856.0\n",
            "Loss after epoch 364: 1920.0\n",
            "Loss after epoch 365: 2048.0\n",
            "Loss after epoch 366: 2248.0\n",
            "Loss after epoch 367: 2104.0\n",
            "Loss after epoch 368: 2112.0\n",
            "Loss after epoch 369: 1976.0\n",
            "Loss after epoch 370: 1640.0\n",
            "Loss after epoch 371: 1800.0\n",
            "Loss after epoch 372: 2032.0\n",
            "Loss after epoch 373: 2032.0\n",
            "Loss after epoch 374: 2072.0\n",
            "Loss after epoch 375: 1912.0\n",
            "Loss after epoch 376: 2008.0\n",
            "Loss after epoch 377: 1992.0\n",
            "Loss after epoch 378: 1896.0\n",
            "Loss after epoch 379: 2008.0\n",
            "Loss after epoch 380: 2016.0\n",
            "Loss after epoch 381: 2104.0\n",
            "Loss after epoch 382: 1920.0\n",
            "Loss after epoch 383: 1912.0\n",
            "Loss after epoch 384: 2152.0\n",
            "Loss after epoch 385: 2536.0\n",
            "Loss after epoch 386: 2136.0\n",
            "Loss after epoch 387: 2024.0\n",
            "Loss after epoch 388: 2192.0\n",
            "Loss after epoch 389: 2096.0\n",
            "Loss after epoch 390: 2048.0\n",
            "Loss after epoch 391: 1832.0\n",
            "Loss after epoch 392: 1568.0\n",
            "Loss after epoch 393: 2016.0\n",
            "Loss after epoch 394: 1736.0\n",
            "Loss after epoch 395: 2128.0\n",
            "Loss after epoch 396: 1872.0\n",
            "Loss after epoch 397: 2080.0\n",
            "Loss after epoch 398: 1744.0\n",
            "Loss after epoch 399: 2088.0\n",
            "Loss after epoch 400: 1952.0\n",
            "Loss after epoch 401: 1832.0\n",
            "Loss after epoch 402: 1952.0\n",
            "Loss after epoch 403: 1688.0\n",
            "Loss after epoch 404: 1824.0\n",
            "Loss after epoch 405: 2096.0\n",
            "Loss after epoch 406: 1640.0\n",
            "Loss after epoch 407: 1936.0\n",
            "Loss after epoch 408: 2128.0\n",
            "Loss after epoch 409: 1704.0\n",
            "Loss after epoch 410: 1792.0\n",
            "Loss after epoch 411: 1912.0\n",
            "Loss after epoch 412: 1816.0\n",
            "Loss after epoch 413: 1728.0\n",
            "Loss after epoch 414: 1952.0\n",
            "Loss after epoch 415: 1952.0\n",
            "Loss after epoch 416: 1904.0\n",
            "Loss after epoch 417: 1872.0\n",
            "Loss after epoch 418: 1992.0\n",
            "Loss after epoch 419: 1760.0\n",
            "Loss after epoch 420: 2080.0\n",
            "Loss after epoch 421: 2008.0\n",
            "Loss after epoch 422: 2080.0\n",
            "Loss after epoch 423: 1952.0\n",
            "Loss after epoch 424: 1912.0\n",
            "Loss after epoch 425: 2048.0\n",
            "Loss after epoch 426: 1912.0\n",
            "Loss after epoch 427: 1672.0\n",
            "Loss after epoch 428: 2024.0\n",
            "Loss after epoch 429: 1960.0\n",
            "Loss after epoch 430: 2160.0\n",
            "Loss after epoch 431: 1952.0\n",
            "Loss after epoch 432: 1792.0\n",
            "Loss after epoch 433: 1840.0\n",
            "Loss after epoch 434: 1880.0\n",
            "Loss after epoch 435: 1944.0\n",
            "Loss after epoch 436: 2008.0\n",
            "Loss after epoch 437: 1920.0\n",
            "Loss after epoch 438: 1880.0\n",
            "Loss after epoch 439: 1800.0\n",
            "Loss after epoch 440: 1792.0\n",
            "Loss after epoch 441: 1968.0\n",
            "Loss after epoch 442: 2024.0\n",
            "Loss after epoch 443: 1784.0\n",
            "Loss after epoch 444: 1808.0\n",
            "Loss after epoch 445: 1976.0\n",
            "Loss after epoch 446: 1864.0\n",
            "Loss after epoch 447: 2056.0\n",
            "Loss after epoch 448: 1760.0\n",
            "Loss after epoch 449: 2264.0\n",
            "Loss after epoch 450: 1760.0\n",
            "Loss after epoch 451: 1904.0\n",
            "Loss after epoch 452: 1824.0\n",
            "Loss after epoch 453: 1976.0\n",
            "Loss after epoch 454: 1704.0\n",
            "Loss after epoch 455: 2128.0\n",
            "Loss after epoch 456: 1760.0\n",
            "Loss after epoch 457: 2008.0\n",
            "Loss after epoch 458: 1504.0\n",
            "Loss after epoch 459: 1552.0\n",
            "Loss after epoch 460: 2000.0\n",
            "Loss after epoch 461: 1928.0\n",
            "Loss after epoch 462: 1864.0\n",
            "Loss after epoch 463: 1696.0\n",
            "Loss after epoch 464: 1808.0\n",
            "Loss after epoch 465: 1912.0\n",
            "Loss after epoch 466: 1856.0\n",
            "Loss after epoch 467: 2016.0\n",
            "Loss after epoch 468: 1664.0\n",
            "Loss after epoch 469: 1664.0\n",
            "Loss after epoch 470: 1696.0\n",
            "Loss after epoch 471: 1736.0\n",
            "Loss after epoch 472: 1784.0\n",
            "Loss after epoch 473: 1728.0\n",
            "Loss after epoch 474: 1640.0\n",
            "Loss after epoch 475: 1816.0\n",
            "Loss after epoch 476: 2032.0\n",
            "Loss after epoch 477: 1840.0\n",
            "Loss after epoch 478: 1904.0\n",
            "Loss after epoch 479: 2008.0\n",
            "Loss after epoch 480: 1776.0\n",
            "Loss after epoch 481: 1944.0\n",
            "Loss after epoch 482: 1912.0\n",
            "Loss after epoch 483: 1848.0\n",
            "Loss after epoch 484: 2000.0\n",
            "Loss after epoch 485: 1912.0\n",
            "Loss after epoch 486: 1912.0\n",
            "Loss after epoch 487: 1808.0\n",
            "Loss after epoch 488: 1672.0\n",
            "Loss after epoch 489: 1728.0\n",
            "Loss after epoch 490: 1816.0\n",
            "Loss after epoch 491: 1856.0\n",
            "Loss after epoch 492: 2128.0\n",
            "Loss after epoch 493: 1832.0\n",
            "Loss after epoch 494: 1704.0\n",
            "Loss after epoch 495: 1944.0\n",
            "Loss after epoch 496: 2112.0\n",
            "Loss after epoch 497: 1792.0\n",
            "Loss after epoch 498: 2000.0\n",
            "Loss after epoch 499: 1784.0\n",
            "Loss after epoch 500: 1824.0\n",
            "Loss after epoch 501: 1816.0\n",
            "Loss after epoch 502: 1944.0\n",
            "Loss after epoch 503: 1824.0\n",
            "Loss after epoch 504: 1872.0\n",
            "Loss after epoch 505: 1816.0\n",
            "Loss after epoch 506: 1968.0\n",
            "Loss after epoch 507: 1808.0\n",
            "Loss after epoch 508: 1864.0\n",
            "Loss after epoch 509: 2080.0\n",
            "Loss after epoch 510: 1720.0\n",
            "Loss after epoch 511: 2048.0\n",
            "Loss after epoch 512: 1928.0\n",
            "Loss after epoch 513: 1832.0\n",
            "Loss after epoch 514: 1832.0\n",
            "Loss after epoch 515: 1872.0\n",
            "Loss after epoch 516: 1728.0\n",
            "Loss after epoch 517: 1864.0\n",
            "Loss after epoch 518: 1888.0\n",
            "Loss after epoch 519: 1912.0\n",
            "Loss after epoch 520: 1848.0\n",
            "Loss after epoch 521: 1776.0\n",
            "Loss after epoch 522: 2080.0\n",
            "Loss after epoch 523: 1840.0\n",
            "Loss after epoch 524: 1720.0\n",
            "Loss after epoch 525: 1480.0\n",
            "Loss after epoch 526: 1704.0\n",
            "Loss after epoch 527: 1848.0\n",
            "Loss after epoch 528: 2128.0\n",
            "Loss after epoch 529: 1864.0\n",
            "Loss after epoch 530: 1736.0\n",
            "Loss after epoch 531: 1944.0\n",
            "Loss after epoch 532: 2040.0\n",
            "Loss after epoch 533: 1760.0\n",
            "Loss after epoch 534: 1872.0\n",
            "Loss after epoch 535: 1856.0\n",
            "Loss after epoch 536: 1848.0\n",
            "Loss after epoch 537: 1840.0\n",
            "Loss after epoch 538: 1992.0\n",
            "Loss after epoch 539: 1760.0\n",
            "Loss after epoch 540: 1872.0\n",
            "Loss after epoch 541: 1808.0\n",
            "Loss after epoch 542: 1856.0\n",
            "Loss after epoch 543: 1808.0\n",
            "Loss after epoch 544: 1984.0\n",
            "Loss after epoch 545: 1776.0\n",
            "Loss after epoch 546: 1752.0\n",
            "Loss after epoch 547: 2000.0\n",
            "Loss after epoch 548: 1880.0\n",
            "Loss after epoch 549: 2184.0\n",
            "Loss after epoch 550: 1824.0\n",
            "Loss after epoch 551: 1944.0\n",
            "Loss after epoch 552: 1696.0\n",
            "Loss after epoch 553: 1856.0\n",
            "Loss after epoch 554: 1640.0\n",
            "Loss after epoch 555: 2000.0\n",
            "Loss after epoch 556: 1976.0\n",
            "Loss after epoch 557: 1752.0\n",
            "Loss after epoch 558: 1592.0\n",
            "Loss after epoch 559: 2248.0\n",
            "Loss after epoch 560: 2088.0\n",
            "Loss after epoch 561: 1856.0\n",
            "Loss after epoch 562: 1632.0\n",
            "Loss after epoch 563: 1680.0\n",
            "Loss after epoch 564: 2112.0\n",
            "Loss after epoch 565: 1848.0\n",
            "Loss after epoch 566: 1736.0\n",
            "Loss after epoch 567: 2000.0\n",
            "Loss after epoch 568: 1888.0\n",
            "Loss after epoch 569: 1952.0\n",
            "Loss after epoch 570: 1872.0\n",
            "Loss after epoch 571: 1664.0\n",
            "Loss after epoch 572: 1768.0\n",
            "Loss after epoch 573: 1928.0\n",
            "Loss after epoch 574: 1952.0\n",
            "Loss after epoch 575: 1648.0\n",
            "Loss after epoch 576: 1824.0\n",
            "Loss after epoch 577: 1888.0\n",
            "Loss after epoch 578: 1880.0\n",
            "Loss after epoch 579: 2120.0\n",
            "Loss after epoch 580: 1856.0\n",
            "Loss after epoch 581: 2120.0\n",
            "Loss after epoch 582: 2016.0\n",
            "Loss after epoch 583: 2216.0\n",
            "Loss after epoch 584: 1968.0\n",
            "Loss after epoch 585: 1960.0\n",
            "Loss after epoch 586: 1952.0\n",
            "Loss after epoch 587: 1872.0\n",
            "Loss after epoch 588: 1576.0\n",
            "Loss after epoch 589: 2000.0\n",
            "Loss after epoch 590: 1568.0\n",
            "Loss after epoch 591: 1352.0\n",
            "Loss after epoch 592: 2240.0\n",
            "Loss after epoch 593: 1696.0\n",
            "Loss after epoch 594: 2056.0\n",
            "Loss after epoch 595: 1816.0\n",
            "Loss after epoch 596: 1848.0\n",
            "Loss after epoch 597: 2064.0\n",
            "Loss after epoch 598: 2120.0\n",
            "Loss after epoch 599: 2200.0\n",
            "Loss after epoch 600: 2112.0\n",
            "Loss after epoch 601: 1656.0\n",
            "Loss after epoch 602: 1776.0\n",
            "Loss after epoch 603: 1856.0\n",
            "Loss after epoch 604: 2080.0\n",
            "Loss after epoch 605: 1736.0\n",
            "Loss after epoch 606: 2120.0\n",
            "Loss after epoch 607: 1952.0\n",
            "Loss after epoch 608: 1816.0\n",
            "Loss after epoch 609: 2080.0\n",
            "Loss after epoch 610: 1776.0\n",
            "Loss after epoch 611: 1776.0\n",
            "Loss after epoch 612: 1952.0\n",
            "Loss after epoch 613: 1960.0\n",
            "Loss after epoch 614: 1960.0\n",
            "Loss after epoch 615: 2168.0\n",
            "Loss after epoch 616: 1696.0\n",
            "Loss after epoch 617: 1576.0\n",
            "Loss after epoch 618: 2008.0\n",
            "Loss after epoch 619: 1760.0\n",
            "Loss after epoch 620: 1952.0\n",
            "Loss after epoch 621: 2048.0\n",
            "Loss after epoch 622: 1944.0\n",
            "Loss after epoch 623: 1784.0\n",
            "Loss after epoch 624: 1896.0\n",
            "Loss after epoch 625: 1904.0\n",
            "Loss after epoch 626: 1800.0\n",
            "Loss after epoch 627: 2016.0\n",
            "Loss after epoch 628: 1656.0\n",
            "Loss after epoch 629: 1816.0\n",
            "Loss after epoch 630: 1600.0\n",
            "Loss after epoch 631: 2200.0\n",
            "Loss after epoch 632: 1712.0\n",
            "Loss after epoch 633: 1824.0\n",
            "Loss after epoch 634: 1976.0\n",
            "Loss after epoch 635: 1776.0\n",
            "Loss after epoch 636: 1960.0\n",
            "Loss after epoch 637: 1864.0\n",
            "Loss after epoch 638: 1896.0\n",
            "Loss after epoch 639: 1952.0\n",
            "Loss after epoch 640: 1880.0\n",
            "Loss after epoch 641: 2096.0\n",
            "Loss after epoch 642: 1936.0\n",
            "Loss after epoch 643: 2008.0\n",
            "Loss after epoch 644: 2072.0\n",
            "Loss after epoch 645: 1824.0\n",
            "Loss after epoch 646: 2104.0\n",
            "Loss after epoch 647: 1888.0\n",
            "Loss after epoch 648: 1920.0\n",
            "Loss after epoch 649: 1840.0\n",
            "Loss after epoch 650: 1776.0\n",
            "Loss after epoch 651: 2008.0\n",
            "Loss after epoch 652: 1720.0\n",
            "Loss after epoch 653: 2064.0\n",
            "Loss after epoch 654: 1784.0\n",
            "Loss after epoch 655: 1936.0\n",
            "Loss after epoch 656: 1984.0\n",
            "Loss after epoch 657: 2040.0\n",
            "Loss after epoch 658: 1904.0\n",
            "Loss after epoch 659: 1832.0\n",
            "Loss after epoch 660: 1616.0\n",
            "Loss after epoch 661: 1800.0\n",
            "Loss after epoch 662: 1968.0\n",
            "Loss after epoch 663: 1752.0\n",
            "Loss after epoch 664: 1544.0\n",
            "Loss after epoch 665: 1808.0\n",
            "Loss after epoch 666: 1976.0\n",
            "Loss after epoch 667: 2072.0\n",
            "Loss after epoch 668: 1904.0\n",
            "Loss after epoch 669: 1800.0\n",
            "Loss after epoch 670: 1912.0\n",
            "Loss after epoch 671: 2000.0\n",
            "Loss after epoch 672: 1824.0\n",
            "Loss after epoch 673: 1896.0\n",
            "Loss after epoch 674: 1768.0\n",
            "Loss after epoch 675: 2064.0\n",
            "Loss after epoch 676: 1872.0\n",
            "Loss after epoch 677: 1744.0\n",
            "Loss after epoch 678: 1816.0\n",
            "Loss after epoch 679: 1816.0\n",
            "Loss after epoch 680: 1928.0\n",
            "Loss after epoch 681: 1912.0\n",
            "Loss after epoch 682: 1840.0\n",
            "Loss after epoch 683: 1856.0\n",
            "Loss after epoch 684: 1800.0\n",
            "Loss after epoch 685: 1848.0\n",
            "Loss after epoch 686: 1888.0\n",
            "Loss after epoch 687: 1632.0\n",
            "Loss after epoch 688: 2032.0\n",
            "Loss after epoch 689: 1840.0\n",
            "Loss after epoch 690: 1888.0\n",
            "Loss after epoch 691: 2024.0\n",
            "Loss after epoch 692: 2072.0\n",
            "Loss after epoch 693: 2000.0\n",
            "Loss after epoch 694: 2088.0\n",
            "Loss after epoch 695: 1792.0\n",
            "Loss after epoch 696: 1608.0\n",
            "Loss after epoch 697: 1856.0\n",
            "Loss after epoch 698: 1920.0\n",
            "Loss after epoch 699: 1872.0\n",
            "Loss after epoch 700: 2120.0\n",
            "Loss after epoch 701: 1720.0\n",
            "Loss after epoch 702: 2000.0\n",
            "Loss after epoch 703: 1800.0\n",
            "Loss after epoch 704: 1800.0\n",
            "Loss after epoch 705: 2016.0\n",
            "Loss after epoch 706: 1880.0\n",
            "Loss after epoch 707: 1912.0\n",
            "Loss after epoch 708: 1968.0\n",
            "Loss after epoch 709: 1904.0\n",
            "Loss after epoch 710: 1648.0\n",
            "Loss after epoch 711: 1920.0\n",
            "Loss after epoch 712: 1920.0\n",
            "Loss after epoch 713: 1984.0\n",
            "Loss after epoch 714: 2040.0\n",
            "Loss after epoch 715: 2168.0\n",
            "Loss after epoch 716: 1688.0\n",
            "Loss after epoch 717: 1912.0\n",
            "Loss after epoch 718: 1808.0\n",
            "Loss after epoch 719: 1888.0\n",
            "Loss after epoch 720: 1968.0\n",
            "Loss after epoch 721: 2000.0\n",
            "Loss after epoch 722: 2008.0\n",
            "Loss after epoch 723: 1976.0\n",
            "Loss after epoch 724: 2000.0\n",
            "Loss after epoch 725: 1864.0\n",
            "Loss after epoch 726: 1912.0\n",
            "Loss after epoch 727: 2016.0\n",
            "Loss after epoch 728: 2008.0\n",
            "Loss after epoch 729: 1696.0\n",
            "Loss after epoch 730: 1472.0\n",
            "Loss after epoch 731: 2056.0\n",
            "Loss after epoch 732: 1824.0\n",
            "Loss after epoch 733: 1904.0\n",
            "Loss after epoch 734: 2080.0\n",
            "Loss after epoch 735: 1904.0\n",
            "Loss after epoch 736: 1888.0\n",
            "Loss after epoch 737: 2016.0\n",
            "Loss after epoch 738: 1976.0\n",
            "Loss after epoch 739: 1776.0\n",
            "Loss after epoch 740: 2176.0\n",
            "Loss after epoch 741: 1936.0\n",
            "Loss after epoch 742: 1720.0\n",
            "Loss after epoch 743: 1872.0\n",
            "Loss after epoch 744: 2112.0\n",
            "Loss after epoch 745: 1952.0\n",
            "Loss after epoch 746: 2040.0\n",
            "Loss after epoch 747: 1984.0\n",
            "Loss after epoch 748: 1976.0\n",
            "Loss after epoch 749: 2104.0\n",
            "Loss after epoch 750: 1800.0\n",
            "Loss after epoch 751: 1952.0\n",
            "Loss after epoch 752: 1944.0\n",
            "Loss after epoch 753: 1992.0\n",
            "Loss after epoch 754: 1696.0\n",
            "Loss after epoch 755: 2088.0\n",
            "Loss after epoch 756: 1744.0\n",
            "Loss after epoch 757: 2008.0\n",
            "Loss after epoch 758: 1896.0\n",
            "Loss after epoch 759: 2128.0\n",
            "Loss after epoch 760: 1824.0\n",
            "Loss after epoch 761: 1936.0\n",
            "Loss after epoch 762: 2096.0\n",
            "Loss after epoch 763: 1992.0\n",
            "Loss after epoch 764: 2024.0\n",
            "Loss after epoch 765: 1936.0\n",
            "Loss after epoch 766: 2160.0\n",
            "Loss after epoch 767: 1640.0\n",
            "Loss after epoch 768: 2040.0\n",
            "Loss after epoch 769: 1808.0\n",
            "Loss after epoch 770: 1840.0\n",
            "Loss after epoch 771: 1696.0\n",
            "Loss after epoch 772: 1952.0\n",
            "Loss after epoch 773: 1856.0\n",
            "Loss after epoch 774: 1992.0\n",
            "Loss after epoch 775: 1744.0\n",
            "Loss after epoch 776: 2008.0\n",
            "Loss after epoch 777: 1872.0\n",
            "Loss after epoch 778: 1848.0\n",
            "Loss after epoch 779: 1864.0\n",
            "Loss after epoch 780: 1968.0\n",
            "Loss after epoch 781: 1960.0\n",
            "Loss after epoch 782: 1912.0\n",
            "Loss after epoch 783: 1944.0\n",
            "Loss after epoch 784: 1976.0\n",
            "Loss after epoch 785: 1960.0\n",
            "Loss after epoch 786: 2104.0\n",
            "Loss after epoch 787: 1928.0\n",
            "Loss after epoch 788: 1880.0\n",
            "Loss after epoch 789: 1824.0\n",
            "Loss after epoch 790: 1896.0\n",
            "Loss after epoch 791: 1984.0\n",
            "Loss after epoch 792: 2088.0\n",
            "Loss after epoch 793: 1664.0\n",
            "Loss after epoch 794: 1728.0\n",
            "Loss after epoch 795: 2080.0\n",
            "Loss after epoch 796: 2040.0\n",
            "Loss after epoch 797: 2216.0\n",
            "Loss after epoch 798: 1744.0\n",
            "Loss after epoch 799: 1856.0\n",
            "Loss after epoch 800: 1648.0\n",
            "Loss after epoch 801: 1688.0\n",
            "Loss after epoch 802: 1656.0\n",
            "Loss after epoch 803: 1896.0\n",
            "Loss after epoch 804: 2072.0\n",
            "Loss after epoch 805: 1912.0\n",
            "Loss after epoch 806: 1984.0\n",
            "Loss after epoch 807: 1864.0\n",
            "Loss after epoch 808: 1904.0\n",
            "Loss after epoch 809: 1984.0\n",
            "Loss after epoch 810: 1800.0\n",
            "Loss after epoch 811: 1736.0\n",
            "Loss after epoch 812: 1944.0\n",
            "Loss after epoch 813: 2080.0\n",
            "Loss after epoch 814: 2032.0\n",
            "Loss after epoch 815: 1808.0\n",
            "Loss after epoch 816: 1856.0\n",
            "Loss after epoch 817: 2128.0\n",
            "Loss after epoch 818: 2008.0\n",
            "Loss after epoch 819: 1888.0\n",
            "Loss after epoch 820: 2096.0\n",
            "Loss after epoch 821: 2000.0\n",
            "Loss after epoch 822: 1824.0\n",
            "Loss after epoch 823: 1960.0\n",
            "Loss after epoch 824: 1800.0\n",
            "Loss after epoch 825: 1872.0\n",
            "Loss after epoch 826: 2096.0\n",
            "Loss after epoch 827: 1768.0\n",
            "Loss after epoch 828: 2152.0\n",
            "Loss after epoch 829: 1752.0\n",
            "Loss after epoch 830: 2056.0\n",
            "Loss after epoch 831: 1808.0\n",
            "Loss after epoch 832: 2208.0\n",
            "Loss after epoch 833: 1872.0\n",
            "Loss after epoch 834: 1840.0\n",
            "Loss after epoch 835: 1624.0\n",
            "Loss after epoch 836: 1840.0\n",
            "Loss after epoch 837: 2184.0\n",
            "Loss after epoch 838: 2232.0\n",
            "Loss after epoch 839: 2232.0\n",
            "Loss after epoch 840: 2064.0\n",
            "Loss after epoch 841: 2104.0\n",
            "Loss after epoch 842: 1880.0\n",
            "Loss after epoch 843: 2008.0\n",
            "Loss after epoch 844: 1976.0\n",
            "Loss after epoch 845: 2008.0\n",
            "Loss after epoch 846: 2120.0\n",
            "Loss after epoch 847: 2128.0\n",
            "Loss after epoch 848: 2008.0\n",
            "Loss after epoch 849: 2064.0\n",
            "Loss after epoch 850: 2000.0\n",
            "Loss after epoch 851: 2128.0\n",
            "Loss after epoch 852: 2008.0\n",
            "Loss after epoch 853: 1952.0\n",
            "Loss after epoch 854: 2040.0\n",
            "Loss after epoch 855: 1936.0\n",
            "Loss after epoch 856: 1888.0\n",
            "Loss after epoch 857: 1928.0\n",
            "Loss after epoch 858: 1928.0\n",
            "Loss after epoch 859: 1872.0\n",
            "Loss after epoch 860: 2056.0\n",
            "Loss after epoch 861: 1960.0\n",
            "Loss after epoch 862: 2016.0\n",
            "Loss after epoch 863: 2160.0\n",
            "Loss after epoch 864: 1888.0\n",
            "Loss after epoch 865: 2040.0\n",
            "Loss after epoch 866: 1864.0\n",
            "Loss after epoch 867: 2040.0\n",
            "Loss after epoch 868: 2168.0\n",
            "Loss after epoch 869: 2104.0\n",
            "Loss after epoch 870: 1864.0\n",
            "Loss after epoch 871: 1976.0\n",
            "Loss after epoch 872: 2160.0\n",
            "Loss after epoch 873: 1816.0\n",
            "Loss after epoch 874: 1584.0\n",
            "Loss after epoch 875: 1912.0\n",
            "Loss after epoch 876: 2072.0\n",
            "Loss after epoch 877: 1920.0\n",
            "Loss after epoch 878: 1792.0\n",
            "Loss after epoch 879: 2200.0\n",
            "Loss after epoch 880: 1880.0\n",
            "Loss after epoch 881: 2000.0\n",
            "Loss after epoch 882: 1824.0\n",
            "Loss after epoch 883: 1880.0\n",
            "Loss after epoch 884: 2024.0\n",
            "Loss after epoch 885: 1800.0\n",
            "Loss after epoch 886: 1800.0\n",
            "Loss after epoch 887: 1880.0\n",
            "Loss after epoch 888: 2088.0\n",
            "Loss after epoch 889: 2144.0\n",
            "Loss after epoch 890: 2072.0\n",
            "Loss after epoch 891: 2256.0\n",
            "Loss after epoch 892: 1968.0\n",
            "Loss after epoch 893: 2128.0\n",
            "Loss after epoch 894: 1960.0\n",
            "Loss after epoch 895: 2192.0\n",
            "Loss after epoch 896: 2056.0\n",
            "Loss after epoch 897: 1848.0\n",
            "Loss after epoch 898: 1856.0\n",
            "Loss after epoch 899: 1608.0\n",
            "Loss after epoch 900: 1920.0\n",
            "Loss after epoch 901: 1976.0\n",
            "Loss after epoch 902: 2184.0\n",
            "Loss after epoch 903: 2048.0\n",
            "Loss after epoch 904: 2216.0\n",
            "Loss after epoch 905: 1768.0\n",
            "Loss after epoch 906: 2120.0\n",
            "Loss after epoch 907: 1880.0\n",
            "Loss after epoch 908: 2072.0\n",
            "Loss after epoch 909: 2152.0\n",
            "Loss after epoch 910: 1944.0\n",
            "Loss after epoch 911: 2152.0\n",
            "Loss after epoch 912: 2296.0\n",
            "Loss after epoch 913: 1888.0\n",
            "Loss after epoch 914: 1648.0\n",
            "Loss after epoch 915: 2192.0\n",
            "Loss after epoch 916: 2104.0\n",
            "Loss after epoch 917: 2056.0\n",
            "Loss after epoch 918: 2072.0\n",
            "Loss after epoch 919: 2048.0\n",
            "Loss after epoch 920: 2216.0\n",
            "Loss after epoch 921: 2072.0\n",
            "Loss after epoch 922: 1968.0\n",
            "Loss after epoch 923: 2048.0\n",
            "Loss after epoch 924: 2000.0\n",
            "Loss after epoch 925: 2008.0\n",
            "Loss after epoch 926: 2080.0\n",
            "Loss after epoch 927: 2032.0\n",
            "Loss after epoch 928: 2072.0\n",
            "Loss after epoch 929: 2208.0\n",
            "Loss after epoch 930: 2280.0\n",
            "Loss after epoch 931: 2104.0\n",
            "Loss after epoch 932: 1968.0\n",
            "Loss after epoch 933: 2320.0\n",
            "Loss after epoch 934: 2328.0\n",
            "Loss after epoch 935: 2208.0\n",
            "Loss after epoch 936: 2040.0\n",
            "Loss after epoch 937: 2008.0\n",
            "Loss after epoch 938: 1968.0\n",
            "Loss after epoch 939: 1984.0\n",
            "Loss after epoch 940: 2304.0\n",
            "Loss after epoch 941: 2264.0\n",
            "Loss after epoch 942: 2304.0\n",
            "Loss after epoch 943: 2056.0\n",
            "Loss after epoch 944: 2032.0\n",
            "Loss after epoch 945: 2000.0\n",
            "Loss after epoch 946: 2368.0\n",
            "Loss after epoch 947: 2040.0\n",
            "Loss after epoch 948: 2056.0\n",
            "Loss after epoch 949: 2104.0\n",
            "Loss after epoch 950: 2136.0\n",
            "Loss after epoch 951: 2352.0\n",
            "Loss after epoch 952: 1960.0\n",
            "Loss after epoch 953: 1992.0\n",
            "Loss after epoch 954: 1976.0\n",
            "Loss after epoch 955: 1944.0\n",
            "Loss after epoch 956: 2160.0\n",
            "Loss after epoch 957: 1856.0\n",
            "Loss after epoch 958: 2192.0\n",
            "Loss after epoch 959: 2000.0\n",
            "Loss after epoch 960: 2304.0\n",
            "Loss after epoch 961: 2360.0\n",
            "Loss after epoch 962: 2240.0\n",
            "Loss after epoch 963: 2112.0\n",
            "Loss after epoch 964: 1952.0\n",
            "Loss after epoch 965: 2296.0\n",
            "Loss after epoch 966: 2312.0\n",
            "Loss after epoch 967: 2104.0\n",
            "Loss after epoch 968: 2024.0\n",
            "Loss after epoch 969: 1992.0\n",
            "Loss after epoch 970: 2464.0\n",
            "Loss after epoch 971: 2200.0\n",
            "Loss after epoch 972: 2448.0\n",
            "Loss after epoch 973: 2256.0\n",
            "Loss after epoch 974: 2200.0\n",
            "Loss after epoch 975: 2272.0\n",
            "Loss after epoch 976: 1872.0\n",
            "Loss after epoch 977: 2080.0\n",
            "Loss after epoch 978: 2416.0\n",
            "Loss after epoch 979: 2104.0\n",
            "Loss after epoch 980: 2032.0\n",
            "Loss after epoch 981: 2264.0\n",
            "Loss after epoch 982: 1984.0\n",
            "Loss after epoch 983: 2240.0\n",
            "Loss after epoch 984: 2088.0\n",
            "Loss after epoch 985: 2128.0\n",
            "Loss after epoch 986: 2360.0\n",
            "Loss after epoch 987: 2200.0\n",
            "Loss after epoch 988: 2040.0\n",
            "Loss after epoch 989: 2104.0\n",
            "Loss after epoch 990: 2104.0\n",
            "Loss after epoch 991: 2184.0\n",
            "Loss after epoch 992: 1920.0\n",
            "Loss after epoch 993: 2152.0\n",
            "Loss after epoch 994: 2336.0\n",
            "Loss after epoch 995: 2256.0\n",
            "Loss after epoch 996: 2544.0\n",
            "Loss after epoch 997: 2280.0\n",
            "Loss after epoch 998: 2008.0\n",
            "Loss after epoch 999: 1856.0\n",
            "Loss after epoch 1000: 2136.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59338010, 115269154)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruebas"
      ],
      "metadata": {
        "id": "Q7MIrb6GSLHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pruebas de interés"
      ],
      "metadata": {
        "id": "EBqPjMwHSNyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabras que MÁS se relacionan con \"matrimonio\"\n",
        "w2v_model.wv.most_similar(positive=[\"matrimonio\"], topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyHJ9iqCSK1J",
        "outputId": "5fd6aefc-50c3-4c84-c8c1-b268a318f147"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('felicidad', 0.33844903111457825),\n",
              " ('proposición', 0.28578025102615356),\n",
              " ('fortuna', 0.2617620527744293),\n",
              " ('recibió', 0.25271058082580566),\n",
              " ('único', 0.2516445219516754),\n",
              " ('casada', 0.24860697984695435),\n",
              " ('seguro', 0.2477155327796936),\n",
              " ('juventud', 0.2438347488641739),\n",
              " ('disgusto', 0.24175982177257538),\n",
              " ('sabía', 0.23984849452972412)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las palabras más relacionadas con \"matrimonio\" incluyen términos como \"felicidad\", \"proposición\" y \"fortuna\", lo que sugiere que el concepto de matrimonio en la novela está vinculado con la idea de bienestar emocional y estabilidad financiera (en el caso de familias que tenian solo hijas mujeres). También aparecen palabras como \"casada\" y \"juventud\", lo que podría reflejar la importancia de la juventud y las expectativas sociales sobre el matrimonio, junto con la referencia al \"disgusto\", que podría señalar las tensiones o desafíos asociados con este tema."
      ],
      "metadata": {
        "id": "gBGdKAYdXUr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabras que MENOS se relacionan con la palabra \"matrimonio\"\n",
        "w2v_model.wv.most_similar(negative=[\"matrimonio\"], topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grI00mK_YoDG",
        "outputId": "380812a7-a2d5-4cf2-910d-5e81d9bfbc22"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bennet—', 0.13732989132404327),\n",
              " ('papá', 0.10017896443605423),\n",
              " ('buena', 0.09543897956609726),\n",
              " ('estos', 0.08292658627033234),\n",
              " ('triste', 0.0802323967218399),\n",
              " ('temer', 0.07793125510215759),\n",
              " ('camino', 0.07552922517061234),\n",
              " ('decidió', 0.07211490720510483),\n",
              " ('realmente', 0.07154206186532974),\n",
              " ('dime', 0.07100652158260345)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las palabras menos relacionadas con \"matrimonio\" incluyen términos como \"bennet—\", \"papá\" y \"buena\", que son más generales y no están directamente vinculadas a la institución del matrimonio. También aparecen palabras como \"triste\" y \"temer\", que reflejan emociones negativas y temores que no suelen asociarse con las ideas positivas o las expectativas sociales que se tienen sobre el matrimonio en la novela."
      ],
      "metadata": {
        "id": "pDwvzFhQaS9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabras que MÁS se relacionan con \"orgullo\"\n",
        "w2v_model.wv.most_similar(positive=[\"orgullo\"], topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60JZcQpzVeIy",
        "outputId": "50695465-29a4-44f2-f2e4-6d8802032494"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('vanidad', 0.33086904883384705),\n",
              " ('afecto', 0.28651466965675354),\n",
              " ('hijo', 0.2750069499015808),\n",
              " ('cualidades', 0.26026734709739685),\n",
              " ('generoso', 0.24782130122184753),\n",
              " ('resentimiento', 0.23501630127429962),\n",
              " ('alto', 0.23411493003368378),\n",
              " ('caso', 0.23210176825523376),\n",
              " ('él', 0.23179791867733002),\n",
              " ('dice', 0.22903448343276978)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las palabras relacionadas con \"orgullo\" como \"vanidad\", \"afecto\" y \"hijo\" indican que el orgullo en la novela está vinculado tanto a la autoimagen como a las relaciones familiares. La presencia de términos como \"generoso\" y \"resentimiento\" refleja el conflicto entre las cualidades personales y las emociones negativas que el orgullo puede generar en las interacciones, mientras que \"alto\" y \"caso\" pueden sugerir distinciones sociales y situaciones donde el orgullo juega un papel importante en los juicios de los personajes."
      ],
      "metadata": {
        "id": "awZsKwmTXXDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabras que MENOS se relacionan con \"orgullo\"\n",
        "w2v_model.wv.most_similar(negative=[\"orgullo\"], topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOTFIOm9Yva5",
        "outputId": "2d795be7-65d9-4be7-cdad-1dc45cff60f0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sobrina', 0.11618072539567947),\n",
              " ('mano', 0.10838272422552109),\n",
              " ('respuesta', 0.1027355045080185),\n",
              " ('apenas', 0.09173315018415451),\n",
              " ('absolutamente', 0.08625943958759308),\n",
              " ('decidida', 0.08038736134767532),\n",
              " ('tuvo', 0.0785980373620987),\n",
              " ('seguida', 0.07518467307090759),\n",
              " ('dirección', 0.07401349395513535),\n",
              " ('pudiera', 0.07262668758630753)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las palabras menos relacionadas con \"orgullo\" incluyen términos como \"sobrina\", \"respuesta\" y \"apenas\", que indican interacciones más neutras o cotidianas. Otras como \"decidida\" y \"tuvo\" sugieren decisiones o acciones que no están impulsadas por el orgullo, sino por otros factores como la determinación o las circunstancias."
      ],
      "metadata": {
        "id": "c2OUcSbnaJIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabras que MÁS se relacionan con \"honor\"\n",
        "w2v_model.wv.most_similar(positive=[\"honor\"], topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gshNQmNCWXfG",
        "outputId": "e8b73981-71a1-4688-b695-a455bb614cb9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bailar', 0.24975107610225677),\n",
              " ('obligado', 0.24886076152324677),\n",
              " ('menor', 0.24734173715114594),\n",
              " ('haciendo', 0.2431022822856903),\n",
              " ('convencido', 0.2407100647687912),\n",
              " ('inconveniente', 0.23863445222377777),\n",
              " ('leer', 0.23241634666919708),\n",
              " ('estima', 0.23049935698509216),\n",
              " ('mano', 0.21645274758338928),\n",
              " ('hiciese', 0.21461975574493408)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las palabras más relacionadas con el término \"honor\" se asocian con palabras como \"bailar\", \"obligado\" y \"menor\", lo que indica que el honor en la novela puede estar relacionado con normas sociales y expectativas de comportamiento en eventos formales, como por ejemplo los bailes. También se encuentran términos como \"convencido\" y \"inconveniente\", lo que sugiere que el honor está conectado con las decisiones y la presión social de la época, mientras que \"estima\" y \"mano\" reflejan la importancia del respeto y el compromiso en las relaciones."
      ],
      "metadata": {
        "id": "OWR92usqXk36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabras que MENOS se relacionan con \"honor\"\n",
        "w2v_model.wv.most_similar(negative=[\"honor\"], topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQd_Nl5dY0l8",
        "outputId": "1d56caa3-3ee4-47ce-976b-b09190b94731"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('menudo', 0.12300035357475281),\n",
              " ('juntos', 0.09283412992954254),\n",
              " ('dentro', 0.09144085645675659),\n",
              " ('encontrado', 0.08942659199237823),\n",
              " ('venir', 0.08901205658912659),\n",
              " ('te', 0.08323408663272858),\n",
              " ('desagradable', 0.08149594068527222),\n",
              " ('casos', 0.0800391137599945),\n",
              " ('salió', 0.07840721309185028),\n",
              " ('desde', 0.07780750095844269)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las palabras menos relacionadas con \"honor\" incluyen términos como \"menudo\", \"juntos\" y \"dentro\", que no tienen una conexión directa con el concepto de honor en el contexto de la novela. Palabras como \"desagradable\" y \"salió\" reflejan situaciones más triviales o negativas que no se asocian con las ideas de respeto y dignidad que el honor implica en la trama."
      ],
      "metadata": {
        "id": "cI3k22uJY0Vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prueba de analogía"
      ],
      "metadata": {
        "id": "1jL3MZ04apag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = w2v_model.wv.most_similar(positive=['elizabeth', 'bingley'], negative=['darcy'], topn=1)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92JGkb8NasaL",
        "outputId": "1c675022-5711-4e91-d3a4-f98fe71307f9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('jane', 0.3704480528831482)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo ha identificado que la palabra más relacionada con la analogía \"Elizabeth + Bingley - Darcy\", es \"Jane\" (0.37 de similitud). Esto tiene sentido dentro del contexto de la novela, ya que Jane Bennet, la hermana de Elizabeth, tiene una relación amorosa con Mr. Bingley. La conexión entre Jane y Bingley es muy similar a la relación entre Elizabeth y Darcy. Es un reflejo de cómo el modelo captura la idea de dos personajes que están en relaciones amorosas paralelas."
      ],
      "metadata": {
        "id": "Nd3zJ5unnHJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = w2v_model.wv.most_similar(positive=['bennet', 'elizabeth'], negative=['lydia'], topn=1)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80d0MBLUjA8e",
        "outputId": "40efe744-3c9d-4187-d4f5-5615fcc0f384"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('señora', 0.45436811447143555)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La analogía \"Bennet + Elizabeth - Lydia\", \"señora\" (0.45 de similitud), podría reflejar la relación general de la madre con sus hijas, en este caso con Lydia, quien representa una hija más rebelde y menos contenida que Elizabeth. Esta respuesta podría interpretarse como un reflejo de la autoridad de la madre sobre sus hijas, especialmente en una sociedad donde la figura materna tenía una gran influencia en las decisiones familiares, como el matrimonio."
      ],
      "metadata": {
        "id": "bLG6IlLUn0Sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualización de agrupaciones de vectores"
      ],
      "metadata": {
        "id": "YnKMtOTxobdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En los gráficos se observará las relaciones entre las palabras. Se utilizá IncrementalPCA para reducir la dimencionalidad de los vectores de palabras para poder observar una mejor cercanía entre ellos."
      ],
      "metadata": {
        "id": "f9m8D062vfSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "def reduce_dimensions_TSNE(model, num_dimensions = 2 ):\n",
        "\n",
        "    vectors = np.asarray(model.wv.vectors)\n",
        "    labels = np.asarray(model.wv.index_to_key)\n",
        "\n",
        "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
        "    vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "    return vectors, labels\n",
        "\n",
        "def reduce_dimensions_IPCA(model, num_dimensions=2, batch_size=100):\n",
        "  vectors = np.asarray(model.wv.vectors)\n",
        "  labels = np.asarray(model.wv.index_to_key)\n",
        "\n",
        "  # Inicializamos IncrementalPCA\n",
        "  ipca = IncrementalPCA(n_components=num_dimensions, batch_size=batch_size)\n",
        "\n",
        "  # Ajustamos y transformamos los vectores\n",
        "  vectors = ipca.fit_transform(vectors)\n",
        "\n",
        "  return vectors, labels"
      ],
      "metadata": {
        "id": "hfzFawsNohb1"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 2D de embeddings con Incremental_PCA\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# Parámetros\n",
        "MAX_WORDS = 100\n",
        "vecs, labels = reduce_dimensions_IPCA(w2v_model)\n",
        "\n",
        "# Plot\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=vecs[:MAX_WORDS, 0],\n",
        "        y=vecs[:MAX_WORDS, 1],\n",
        "        mode='markers+text',\n",
        "        text=labels[:MAX_WORDS],\n",
        "        textposition=\"top center\",\n",
        "        textfont=dict(size=9),\n",
        "        marker=dict(size=6, color='blue', opacity=0.7),\n",
        "        hoverinfo='text',\n",
        "    )\n",
        ")\n",
        "\n",
        "# Formato del gráfico\n",
        "fig.update_layout(\n",
        "    title=\"Proyección 2D - Embeddings con Incremental PCA\",\n",
        "    xaxis_title=\"Componente 1\",\n",
        "    yaxis_title=\"Componente 2\",\n",
        "    template=\"plotly_white\",\n",
        "    showlegend=False,\n",
        "    autosize=True,\n",
        "    margin=dict(l=40, r=40, t=40, b=40),  # Márgenes ajustados\n",
        ")\n",
        "\n",
        "# Mostrar el gráfico\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "ufKNtFKlwXDi",
        "outputId": "cf2f7359-db54-4f70-9bd9-64f20b5906c3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"83d8122d-77d9-432b-a4f3-397e4f514dd4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"83d8122d-77d9-432b-a4f3-397e4f514dd4\")) {                    Plotly.newPlot(                        \"83d8122d-77d9-432b-a4f3-397e4f514dd4\",                        [{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"blue\",\"opacity\":0.7,\"size\":6},\"mode\":\"markers+text\",\"text\":[\"de\",\"que\",\"la\",\"a\",\"y\",\"no\",\"en\",\"su\",\"el\",\"se\",\"con\",\"lo\",\"por\",\"le\",\"pero\",\"los\",\"para\",\"elizabeth\",\"una\",\"más\",\"las\",\"un\",\"al\",\"había\",\"me\",\"darcy\",\"sus\",\"es\",\"señor\",\"como\",\"tan\",\"del\",\"era\",\"si\",\"muy\",\"bingley\",\"señora\",\"todo\",\"él\",\"mi\",\"ella\",\"cuando\",\"jane\",\"bennet\",\"estaba\",\"usted\",\"sin\",\"ser\",\"wickham\",\"señorita\",\"ni\",\"ya\",\"ha\",\"nada\",\"collins\",\"casa\",\"qué\",\"o\",\"tenía\",\"aunque\",\"fue\",\"dos\",\"te\",\"yo\",\"bien\",\"mucho\",\"hermana\",\"lydia\",\"catherine\",\"todos\",\"después\",\"podía\",\"vez\",\"sólo\",\"les\",\"nunca\",\"hasta\",\"tiempo\",\"dijo\",\"habría\",\"poco\",\"familia\",\"porque\",\"sido\",\"siempre\",\"menos\",\"todas\",\"pues\",\"antes\",\"he\",\"ver\",\"lady\",\"puede\",\"así\",\"esta\",\"mejor\",\"hacer\",\"otra\",\"hubiese\",\"—dijo\"],\"textfont\":{\"size\":9},\"textposition\":\"top center\",\"x\":[-0.23922906072400485,-0.02851526272835432,-0.3545677563315769,-0.24246323197957512,-0.317811148585216,0.06154419094309206,-0.2394620967095537,-0.34968724690782277,-0.26755290081592936,-0.4019650042847511,-0.3023628449766317,0.042377705949007846,-0.19894821892478803,-0.28020053897290403,-0.08756339062208769,-0.25498979272135514,-0.18174584754783377,-0.6901253721720689,-0.14203225376893014,-0.10464020952131492,-0.2587724823428452,-0.16371536627622263,-0.4661933726744406,-0.6617918013510592,0.6219372115577299,-0.30461425192467523,-0.40508620744611357,0.8452927155273299,0.023905981095613663,0.04499719107288412,0.04146630284441981,-0.3038667356907016,-0.5287938981542469,0.3396375683493129,0.05785352891171512,-0.25314795173826493,-0.4455502856961986,-0.06044233545112478,-0.03317210172819325,0.5839046038627437,-0.41629872030639337,-0.32722608074652804,-0.43215175849741727,-0.4589301579466299,-0.8230068716027276,0.6982734285052158,-0.27163035434903515,0.3479706273234742,-0.1007336604834561,-0.11391267233504275,0.05301258532447713,-0.06299333825366739,1.0282838628731215,0.13846645346476588,-0.3086679828770445,-0.39760617873846527,0.4838783132767194,-0.03760876922479335,-0.5680020004581904,-0.15202062523599913,-0.5732899681264907,-0.3792809636624913,1.2046729835054664,0.872599982296578,0.3622146683115202,0.19280023358775333,-0.13141856647815664,-0.19350288115795955,-0.3138526679198312,-0.1896240844978212,-0.7359400923415459,-0.8050135252411174,-0.1993446688060474,0.0579508451734944,-0.77409741521718,0.20794744182033825,-0.5471669745891383,-0.33247353755223247,-0.6614925132999315,0.1558483729689838,-0.2508846687112447,-0.26903244941249077,0.28542556212109205,0.207793791504999,0.18973401160739423,0.023137331415645154,-0.2416184726401885,-0.06319335615011763,-0.2574195173766699,1.1496082049592986,-0.2941301327216744,-0.30651266143658124,0.7810600230259469,0.4441620653077496,0.019621680063297856,0.3818749583017814,0.03591358152845768,-0.15207399177132191,-0.04860232463926541,1.0710653566251673],\"y\":[0.2966189193364816,0.23719267715385595,0.1480674902855163,0.1619851816441979,0.21461438519599,0.20000653270056096,0.21689310177603308,0.26062040313601526,0.2459559271238203,0.14221922673241627,0.20779322807907846,0.20249006846361378,0.18154660457115238,0.1292478454356265,0.2077467490240797,0.40043423720413396,0.2484673674515751,-0.04523227826149093,0.2321775090875345,0.31038274207456046,0.3533477818220143,0.10907611062072459,0.20384101852974895,0.26349826917437363,0.2031314718883729,0.050277502671216,0.3814876094857804,0.1088078941196983,0.09534234324899951,0.30458192676909185,0.23901787114734552,0.25557993846907906,0.3470508021128428,0.1942685936843333,0.23387101371269747,0.14247741273340392,0.15949798943615334,0.21960323829929906,0.11252215556722137,0.31389604792287734,-0.005631382641328481,-0.06345403319957107,-0.07047882210296105,0.19198370853768648,0.0896672432716096,0.216765629652116,0.23968061442744404,0.34851844412005667,0.3578083505216439,-0.29705758856838305,0.16402017897134386,0.060231339322288496,0.06120472017595485,0.019346800518758978,0.052269967534824435,0.15512507689572141,-0.22565365104014914,0.3907799676191067,0.33253499756124216,0.3264567392166565,0.21311498246574082,0.5431997965764401,-0.28892104554195475,0.014923805876082441,0.15833198561733827,0.23800597077706348,0.2546413380512836,0.2004445070904481,0.1457177392918469,0.23630062382607453,0.2436349701614726,0.23486549310148253,-0.10687519653415592,0.4369790883403113,0.3533862129444252,-0.06707098301459832,0.035270846958822766,0.18150855605385022,-0.32059583059802177,0.1975223909367821,0.08778166289336825,0.5050946911250594,0.17556950741106853,0.4498012589307213,0.2592397088328265,0.5593233057169512,0.21486195654197757,0.47606187696553415,0.20659732656926874,0.28085040114613674,0.2609231916052106,0.14303389649154566,0.2740703093302731,-0.2036928842290113,0.19633709828413007,0.33464227232644006,0.06333980589608493,0.15303619659416756,0.2040863853928446,-0.4468782828567094],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"margin\":{\"l\":40,\"r\":40,\"t\":40,\"b\":40},\"title\":{\"text\":\"Proyección 2D - Embeddings con Incremental PCA\"},\"xaxis\":{\"title\":{\"text\":\"Componente 1\"}},\"yaxis\":{\"title\":{\"text\":\"Componente 2\"}},\"showlegend\":false,\"autosize\":true},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('83d8122d-77d9-432b-a4f3-397e4f514dd4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Del siguiente grafico se puede concluir que:\n",
        "\n",
        "\n",
        "  *   Los personajes como \"wickham\", \"darcy\", \"jane\" y \"bingley\" están ubicados en diferentes zonas del gráfico, lo que podría reflejar las diferentes interacciones que tienen entre sí a lo largo de la novela.\n",
        "  *   \"Darcy\" y \"bingley\", que tienen relaciones importantes con \"jane\" y \"elizabeth\", están relativamente cerca unas de otras, pero no forman un grupo cohesivo. Indicando que, en los embeddings de palabras, sus contextos no son necesariamente coincidentes en todos los casos, aunque en la trama estén vinculadas a través de relaciones complejas.\n",
        "  * Hay palabras como \"señora\", \"señorita\" y \"familia\", pero su relación no está tan próxima como podría esperarse. Esto sugiere que, aunque estas palabras se utilizan en situaciones de relaciones de clase social y matrimonio, los embeddings las tratan como términos más contextuales, dispersos y vinculados a un conjunto más amplio de temas.\n",
        "  * \"Elizabeth\" y \"bennet\" están en posiciones separadas, lo que puede reflejar que, aunque están estrechamente relacionadas en el contexto de la novela, en términos de sus embeddings de palabras no son las más cercanas entre sí.\n"
      ],
      "metadata": {
        "id": "femRHmYsxQOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráficos 3D de embeddings con Incremental_PCA\n",
        "\n",
        "vecs, labels = reduce_dimensions_IPCA(w2v_model,3)\n",
        "\n",
        "# Plot\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter3d(\n",
        "        x=vecs[:MAX_WORDS, 0],\n",
        "        y=vecs[:MAX_WORDS, 1],\n",
        "        z=vecs[:MAX_WORDS, 2],\n",
        "        mode='markers+text',\n",
        "        marker=dict(\n",
        "            size=4,\n",
        "            color='blue',\n",
        "            opacity=0.7\n",
        "        ),\n",
        "        text=labels[:MAX_WORDS],\n",
        "        textposition='top center',\n",
        "        textfont=dict(size=9),\n",
        "        hoverinfo='text',\n",
        "    )\n",
        ")\n",
        "\n",
        "# Personalizar el diseño del gráfico\n",
        "fig.update_layout(\n",
        "    title=\"Proyección 3D - Embeddings con Incremental PCA\",\n",
        "    scene=dict(\n",
        "        xaxis_title=\"Componente 1\",\n",
        "        yaxis_title=\"Componente 2\",\n",
        "        zaxis_title=\"Componente 3\",\n",
        "    ),\n",
        "    template=\"plotly_white\",\n",
        "    showlegend=False,  # Ocultar leyenda\n",
        "    margin=dict(l=0, r=0, t=40, b=0),  # Márgenes ajustados\n",
        ")\n",
        "\n",
        "# Mostrar el gráfico\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "rp06HOs-xQlW",
        "outputId": "4cff5660-6f72-489c-e6c7-9216c442bdd3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"5a65004e-5ddc-4c0b-a2db-8731810b2d42\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5a65004e-5ddc-4c0b-a2db-8731810b2d42\")) {                    Plotly.newPlot(                        \"5a65004e-5ddc-4c0b-a2db-8731810b2d42\",                        [{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"blue\",\"opacity\":0.7,\"size\":4},\"mode\":\"markers+text\",\"text\":[\"de\",\"que\",\"la\",\"a\",\"y\",\"no\",\"en\",\"su\",\"el\",\"se\",\"con\",\"lo\",\"por\",\"le\",\"pero\",\"los\",\"para\",\"elizabeth\",\"una\",\"más\",\"las\",\"un\",\"al\",\"había\",\"me\",\"darcy\",\"sus\",\"es\",\"señor\",\"como\",\"tan\",\"del\",\"era\",\"si\",\"muy\",\"bingley\",\"señora\",\"todo\",\"él\",\"mi\",\"ella\",\"cuando\",\"jane\",\"bennet\",\"estaba\",\"usted\",\"sin\",\"ser\",\"wickham\",\"señorita\",\"ni\",\"ya\",\"ha\",\"nada\",\"collins\",\"casa\",\"qué\",\"o\",\"tenía\",\"aunque\",\"fue\",\"dos\",\"te\",\"yo\",\"bien\",\"mucho\",\"hermana\",\"lydia\",\"catherine\",\"todos\",\"después\",\"podía\",\"vez\",\"sólo\",\"les\",\"nunca\",\"hasta\",\"tiempo\",\"dijo\",\"habría\",\"poco\",\"familia\",\"porque\",\"sido\",\"siempre\",\"menos\",\"todas\",\"pues\",\"antes\",\"he\",\"ver\",\"lady\",\"puede\",\"así\",\"esta\",\"mejor\",\"hacer\",\"otra\",\"hubiese\",\"—dijo\"],\"textfont\":{\"size\":9},\"textposition\":\"top center\",\"x\":[-0.21330319688194763,-0.00445042844816626,-0.33150250878708276,-0.21915051094736168,-0.29454473329071035,0.08553989084362987,-0.21438571133139833,-0.3245321639649894,-0.24465324348299416,-0.3790362485089201,-0.28012939514805174,0.0657393746771989,-0.1747489832499699,-0.2597199218343704,-0.06310082704093453,-0.23178066750134366,-0.158145650825688,-0.6706523825131303,-0.12003467205489435,-0.08044766278828253,-0.23319677243311893,-0.14192335846697113,-0.4440326104929863,-0.6402424336189689,0.6483307260732964,-0.2847777463135071,-0.37820298055605334,0.8667793333840237,0.04011277051829448,0.06828001951695883,0.06403427848928944,-0.281528132647877,-0.5057872219964012,0.3639279974506812,0.08150713900801737,-0.2338397181908343,-0.42841851337212955,-0.039351734649078186,-0.012138460509142732,0.60908380665908,-0.3931419536752789,-0.30927181516387575,-0.41027973296681025,-0.4420907225962521,-0.8061831242330482,0.7206097152719517,-0.24918505997309698,0.36870591347021575,-0.08337590404880926,-0.09789079186086655,0.07539280414010896,-0.04158933129512091,1.0483102072140145,0.15862180867364256,-0.2912872068618571,-0.37878075603112377,0.494850750100896,-0.00980291724597876,-0.5478162709253698,-0.12518970647729225,-0.5562128350772448,-0.3563523947561548,1.2136055441714666,0.8926473932648773,0.3842943118907513,0.21354915435272898,-0.11396176366562713,-0.173270432427666,-0.29956536420320695,-0.17027501523898364,-0.7215175600618007,-0.7804735381070027,-0.1844136326014445,0.07924309725956616,-0.7621552108698524,0.22677872943457222,-0.5338657601212579,-0.3103324280484402,-0.6695285841492016,0.18107242711928423,-0.22876712436177335,-0.2507396749309,0.3050447530043183,0.23277642984813895,0.21318539477983495,0.039572866512132654,-0.21894544503926866,-0.03936222401519135,-0.23569207082243004,1.1680593161399908,-0.27696992394473136,-0.292516546055349,0.8057530274404634,0.46373461403465027,0.043906377348503944,0.40278470214300693,0.05441481561389935,-0.13432421903378738,-0.022792162920868858,1.0579465238052657],\"y\":[1.0372278322947588,0.9872498394670387,0.9025095485010441,0.8872007551788255,0.9700579131979115,0.9868543091007806,0.9532567403909936,1.0756546345004705,0.8775166395200598,0.8638056359933579,0.9805744519473985,0.9719086957508086,1.0264791876287045,0.9542764545422597,1.007432256879525,0.9153158630400146,0.8921507148424158,0.8252136692240619,0.9308958666395516,1.0295243811473422,0.8886194216268146,0.8706463386193382,0.8302108437726511,1.0504243905692658,0.929884399949709,0.9108187834931242,1.1298131381314414,0.8949309657381996,0.5750066507731243,1.0754019542008302,1.0164707922561138,0.8927710997439662,1.1170893201561385,0.869782030683522,1.0042252308240054,0.7699738707769374,0.5929124358508889,0.9445213019524717,0.9173469343935834,0.9520339685075145,0.9458016072603284,0.5980630424662248,0.7827688466180138,0.5763816957811544,0.7946796055267988,0.8161184409361608,0.9551222207879081,1.0880701204630308,0.9798220221920321,0.5532530479502772,0.9373021480255794,0.7161594170878786,0.5409853721337378,0.7671362345865687,0.5204728442807448,0.45482620672206486,0.3997034599373701,0.8548338475304431,0.8697226345785569,1.2073764614471554,0.6911007722223415,0.712856761296828,0.16774920640706115,0.6773171014446562,0.6131821253086394,0.7771588702486253,0.8932630051195743,0.7144827240167075,0.574921864859347,0.8249701133654993,0.48194150498943367,1.1609361418338537,0.5482287239884588,0.9315002045850017,0.6010527808190597,0.9219889101331198,0.430653978834838,0.7105373034784671,0.14302162666290708,0.886994333430837,0.8081787182414663,0.9793446870473295,0.7356103257217834,1.081590181205158,0.9480641019489596,0.9314460582880157,0.875498266335448,1.0028751540527399,0.6115234732813457,0.811388242999929,0.7786452310509362,0.55928902328926,0.7598649477293083,0.48277860584232335,0.7725225716078439,0.8071523024056524,0.673518565212851,0.7319065119267689,0.8117011691627496,-0.24029958903183224],\"z\":[0.5525540310498177,0.5704104530047337,0.6258452480842827,0.6229871323229029,0.575257189080874,0.6010935433737506,0.5932938345257027,0.5861308827411353,0.5349009747406615,0.6321096790127545,0.5669902804673495,0.5843721902484859,0.6311582004886748,0.6141770023046719,0.6102794868302499,0.3321199157869881,0.5512486141955756,0.7137038920727381,0.531350744805711,0.4832083282459503,0.4124853922838157,0.626398330975719,0.5449438615385215,0.49557928944431395,0.6197952710142169,0.6392378993546265,0.4529958452568708,0.6325042619136677,0.5198157675029986,0.5363196837325583,0.5777821320060073,0.5078235404930238,0.4168268647357759,0.5562597802377736,0.5428274731997658,0.545762552790111,0.4350455260491714,0.5295381496908319,0.5949715848924404,0.5043965230720847,0.7600669266160778,0.7036970793922408,0.7592048649410269,0.38835025498462394,0.5410772035090181,0.531103713206363,0.5319240620901471,0.40575608441090266,0.3873844312948682,0.8655520116978099,0.5315163549362485,0.6132833663846335,0.5307018815432472,0.5767615895158282,0.500021584069695,0.45602867926046486,0.6447829457309286,0.31113310292798724,0.3036060671279973,0.49822254267994176,0.40867874946090865,0.08229834103110178,0.6078136640147562,0.6217991028538765,0.5157975431499582,0.4681518788989855,0.3498821073209031,0.4244930643101661,0.41642462940242797,0.34326080150445937,0.24195175973948368,0.5533069880122607,0.6092982124832358,0.25128143036028866,0.1746666216330106,0.716241437290963,0.4989415365619221,0.5637158115952574,0.6019560063482512,0.557685370639968,0.5870964783099925,0.20706892521366155,0.4683016883436367,0.26284880525892806,0.48448128589685924,0.08852189852925996,0.4968213957312221,0.3335204471460695,0.5078290969531433,0.2829222689430628,0.4034816412361699,0.42105112335149036,0.31402604762642494,0.8325534494923681,0.5731546120780742,0.3535517457924293,0.5334335537040631,0.4342421626574938,0.4413382079012356,0.579159997420343],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"margin\":{\"l\":0,\"r\":0,\"t\":40,\"b\":0},\"title\":{\"text\":\"Proyección 3D - Embeddings con Incremental PCA\"},\"scene\":{\"xaxis\":{\"title\":{\"text\":\"Componente 1\"}},\"yaxis\":{\"title\":{\"text\":\"Componente 2\"}},\"zaxis\":{\"title\":{\"text\":\"Componente 3\"}}},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5a65004e-5ddc-4c0b-a2db-8731810b2d42');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al agregar una dimensión (componente) más, se pueden observar mejor las cercanías entre palabras."
      ],
      "metadata": {
        "id": "IcO2B2TVyC55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colclusiones"
      ],
      "metadata": {
        "id": "NQq9VuH42Gvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo de embeddings ha logrado reflejar de manera precisa el contexto semántico de Orgullo y prejuicio, evidenciando cómo los términos y conceptos clave están conectados entre sí. Palabras como \"señor\", \"familia\", \"hermana\", \"Bennet\", \"Elizabeth\", \"Jane\" y \"señorita\" destacan las dinámicas familiares y sociales que dominan la trama, enfocándose en temas como el matrimonio y las relaciones de clase en la sociedad de la época.\n",
        "\n",
        "Las pruebas de analogías han desvelado asociaciones semánticas reveladoras. Por ejemplo, la relación entre \"Elizabeth + Bingley - Darcy\" que da como resultado \"Jane\", y la analogía \"Bennet + Elizabeth - Lydia\" que da como \"señora\", reflejan las complejas interacciones entre los personajes y las tensiones dentro de la familia Bennet. Estos resultados indican que el modelo captura las relaciones emocionales y sociales fundamentales que mueven la narrativa de la novela.\n",
        "\n",
        "La reducción de dimensionalidad utilizando Incremental PCA ha permitido visualizar cómo se agrupan las palabras en función de su significado y contexto. Palabras relacionadas con los personajes, sus acciones y el desarrollo de la trama, como \"dijo\", \"estaba\", \"tenía\" y \"era\", se agrupan de manera coherente, mientras que términos como \"señor\" y \"señorita\" se dispersan, lo que refleja las distinciones de clase y estatus social que son cruciales en el texto. Esta visualización refuerza cómo las relaciones interpersonales y los valores sociales se entrelazan de manera significativa a lo largo de la novela."
      ],
      "metadata": {
        "id": "eWbfMAmJ2Jl6"
      }
    }
  ]
}
